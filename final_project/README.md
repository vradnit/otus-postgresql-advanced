# Сравнение производительности кластера Citus на базе Patoni и ArenadataDB на большом объеме данных


## Описание цели, схемы и плана тестирования

### Цель: сравнить производительность аналитических запросов кластеров "Citus на базе Patoni" и "ArenadataDB"

### Схема тестирования:
- Версия ОS: Ubuntu 22.04
- Конфигурация нод:  
  CPU/8, Mem/16GB, SSD/30GB
- Распределение нод:
  - 3 для мастер нод
  - 6 для воркер нод
- Oграничения:
  - используется виртуализация vmware
  - под капотом один SSD диск на все ВМ
- Датасет для тестирования:  
  [https://zenodo.org/records/7923702](https://zenodo.org/records/7923702)  
  ( открытые данные авиаперелетов с 2019 по 2022 год )



## Описание установки "кластера Citus на базе Patoni"

### Схема стенда "кластера Citus на базе Patoni"
![Схема стенда кластера Citus на базе Patoni](images/schema_citus_patroni.png "Схема стенда кластера Citus на базе Patoni")

### На все ноды в /etc/hosts добавляем 
```
192.168.0.231  c-master1
192.168.0.232  c-master2
192.168.0.233  c-master3
192.168.0.234  c-worker1-1
192.168.0.235  c-worker1-2
192.168.0.236  c-worker2-1
192.168.0.237  c-worker2-2
192.168.0.238  c-worker3-1
192.168.0.239  c-worker3-2
```

### На мастер нодах устанавливаем etcd
```
# apt-get install -y etcd
# systemctl stop etcd
```

### Создаем конфиг etcd на каждой мастер ноде.
Пример конфига с ноды c-master1
```
root@c-master1:~# cat /etc/default/etcd 
ETCD_NAME="c-master1"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://c-master1:2379"
ETCD_LISTEN_PEER_URLS="http://0.0.0.0:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://c-master1:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd_claster"
ETCD_INITIAL_CLUSTER="c-master1=http://c-master1:2380,c-master2=http://c-master2:2380,c-master3=http://c-master3:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_ENABLE_V2="true"
ETCDCRL_API=2
```

### Стартуем etcd на всех мастер нодах и проверяем статус
```
root@c-master1:~# systemctl start etcd

root@c-master1:~# etcdctl member list
5184d67c0e2764e1: name=c-master2 peerURLs=http://c-master2:2380 clientURLs=http://c-master2:2379 isLeader=true
b4d321b6dd227fc9: name=c-master3 peerURLs=http://c-master3:2380 clientURLs=http://c-master3:2379 isLeader=false
bc691ad35811d924: name=c-master1 peerURLs=http://c-master1:2380 clientURLs=http://c-master1:2379 isLeader=false

root@c-master1:~# etcdctl cluster-health
member 5184d67c0e2764e1 is healthy: got healthy result from http://c-master2:2379
member b4d321b6dd227fc9 is healthy: got healthy result from http://c-master3:2379
member bc691ad35811d924 is healthy: got healthy result from http://c-master1:2379
cluster is healthy
```


### На всех нодах устанавливаем демон tuned и создаем профиль для тюнинга ОС под нагрузку сервиса postgresql
```
root@c-master1:~# apt-get install tuned
root@c-master1:~# > /usr/lib/tuned/postgresql/tuned.conf
root@c-master1:~# vim.tiny /usr/lib/tuned/postgresql/tuned.conf 
root@c-master1:~# cat /usr/lib/tuned/postgresql/tuned.conf
[main]
summary=Optimize for PostgreSQL RDBMS
include=throughput-performance
[sysctl]
vm.swappiness = 5
vm.dirty_background_ratio = 10
vm.dirty_ratio = 40
vm.dirty_expire_centisecs = 3000
vm.dirty_writeback_centisecs = 500
kernel.shmmax = 18446744073692700000
kernel.shmall = 18446744073692700000
kernel.shmmni = 4096
kernel.sem = 250 512000 100 2048
fs.file-max = 312139770
fs.aio-max-nr = 1048576
net.ipv4.ip_local_port_range = 2048 65499
# Permits sockets in the time-wait state to be reused for new connections:
net.ipv4.tcp_tw_reuse = 1
net.core.netdev_budget = 1024
net.core.netdev_max_backlog = 2048
net.core.rmem_default = 262144
net.core.rmem_max = 4194304
net.core.wmem_default = 262144
net.core.wmem_max = 1048576
kernel.panic_on_oops = 1
# We don't need NUMA balancing in this box:
kernel.numa_balancing = 0
# Used if not defined by the service:
net.core.somaxconn = 4096
# Other parameters to override throughput-performance template
net.ipv4.tcp_rmem = 4096 87380 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216
net.ipv4.tcp_window_scaling = 1
net.netfilter.nf_conntrack_max = 250000
net.ipv4.tcp_max_syn_backlog=4096
[vm]
transparent_hugepages=never


root@c-master1:~# tuned-adm active ; tuned-adm profile postgresql ;tuned-adm active
```


### Получение конфигурации для тюнинга postgresql 
Используя онлайн конфигуратор https://www.pgconfig.org  
и характеристики нод, получаем:
```
# https://www.pgconfig.org
# Generated by PGConfig 3.1.4 (1fe6d98dedcaad1d0a114617cfd08b4fed1d8a01)
# https://api.pgconfig.org/v1/tuning/get-config?format=conf&&log_format=csvlog&max_connections=100&pg_version=16&environment_name=DW&total_ram=16GB&cpus=8&drive_type=SSD&arch=x86-64&os_type=linux

# Memory Configuration
shared_buffers = 4GB
effective_cache_size = 12GB
work_mem = 82MB
maintenance_work_mem = 819MB

# Checkpoint Related Configuration
min_wal_size = 2GB
max_wal_size = 3GB
checkpoint_completion_target = 0.9
wal_buffers = -1

# Network Related Configuration
listen_addresses = '*'
max_connections = 100

# Storage Configuration
random_page_cost = 1.1
effective_io_concurrency = 200

# Worker Processes Configuration
max_worker_processes = 8
max_parallel_workers_per_gather = 2
max_parallel_workers = 2
```


### Устанавливаем postgresql и patroni на всех нодах кластера и создаем директорию для базы данных:
(на всех нодах)
```
root@c-master1:~# echo "deb [arch=amd64] http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/postgresql.list
root@c-master1:~# wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
root@c-master1:~# curl https://install.citusdata.com/community/deb.sh > add-citus-repo.sh
root@c-master1:~# . ./add-citus-repo.sh
root@c-master1:~# apt-get install -y postgresql-16 postgresql-16-citus-13.0 postgresql-16-postgis-3 patroni python3-etcd
root@c-master1:~# systemctl stop postgresql.service ; systemctl disable postgresql.service
root@c-master1:~# systemctl enable patroni ; systemctl stop patroni
root@c-master1:~# mkdir -p /data/pg/ ; chown postgres:postgres /data/pg ; chmod go-rx /data/pg
```

### Создаем конфигурационный файл /etc/patroni/config.yml
```
scope: patroni
name: c-master1
restapi:
  listen: 0.0.0.0:8008
  connect_address: c-master1:8008
etcd:
  hosts: c-master1:2379,c-master2:2379,c-master3:2379
citus:
  group: 0
  database: otus
bootstrap:
  dcs:
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576
    postgresql:
      use_pg_rewind: true
      parameters:
        checkpoint_completion_target: 0.9
        effective_cache_size: 12GB
        effective_io_concurrency: 200
        maintenance_work_mem: 819MB
        max_connections: 100
        max_parallel_workers: 2
        max_parallel_workers_per_gather: 2
        max_wal_size: 3GB
        max_worker_processes: 8
        min_wal_size: 2GB
        random_page_cost: 1.1
        shared_buffers: 4GB
        wal_buffers: -1
        work_mem: 82MB
  initdb:
  - encoding: UTF8
  - data-checksums
  pg_hba:
  - host replication replicator 0.0.0.0/0 md5
  - host all all 0.0.0.0/0 md5
  users:
    admin:
      password: qwerty
      options:
        - createrole
        - createdb
postgresql:
  listen: 0.0.0.0:5432
  connect_address: c-master1:5432
  data_dir: /data/pg/
  bin_dir: /usr/lib/postgresql/16/bin/
  pgpass: /tmp/pgpass0
  authentication:
    replication:
      username: replicator
      password: qwerty
    superuser:
      username: postgres
      password: qwerty
    rewind:
      username: rewind_user
      password: qwerty
  parameters:
    unix_socket_directories: '.'
tags:
    nofailover: false
    noloadbalance: false
    clonefrom: false
    nosync: false
```
, где
- name, restapi.connect_address, postgresql.connect_address - меняем на имя ноды
- citus.group - устанавливаем 0 на мастерах, 1 на c-worker1-[1,2], 2 на c-worker2-[1,2], 3 на c-worker3-[1,2]
- bootstrap.dcs.postgresql.parameters - добавляем параметры полученные с https://www.pgconfig.org


### Стартуем patroni на мастер нодах
```
root@c-master1:~#  systemctl stаp patroni
```
### Проверяем статус patroni на мастер нодах
```
root@c-master1:~# patronictl --config-file=/etc/patroni/config.yml list
+ Citus cluster: patroni -------+----------------+-----------+----+-----------+
| Group | Member    | Host      | Role           | State     | TL | Lag in MB |
+-------+-----------+-----------+----------------+-----------+----+-----------+
|     0 | c-master1 | c-master1 | Leader         | running   |  1 |           |
|     0 | c-master2 | c-master2 | Quorum Standby | streaming |  1 |         0 |
|     0 | c-master3 | c-master3 | Quorum Standby | streaming |  1 |         0 |
+-------+-----------+-----------+----------------+-----------+----+-----------+
```

### Затем стартуем patroni на воркер нодах 
сначала все с индексом "1", затем с индексом "2",  
и проверяем статус нод кластера:
```
root@c-worker1-1:~#  systemctl stаrt patroni

root@c-worker1-1:~# patronictl --config-file=/etc/patroni/config.yml list
+ Citus cluster: patroni -----------+----------------+-----------+----+-----------+
| Group | Member      | Host        | Role           | State     | TL | Lag in MB |
+-------+-------------+-------------+----------------+-----------+----+-----------+
|     0 | c-master1   | c-master1   | Leader         | running   |  1 |           |
|     0 | c-master2   | c-master2   | Quorum Standby | streaming |  1 |         0 |
|     0 | c-master3   | c-master3   | Quorum Standby | streaming |  1 |         0 |
|     1 | c-worker1-1 | c-worker1-1 | Leader         | running   |  1 |           |
|     1 | c-worker1-2 | c-worker1-2 | Quorum Standby | streaming |  1 |         0 |
|     2 | c-worker2-1 | c-worker2-1 | Leader         | running   |  1 |           |
|     2 | c-worker2-2 | c-worker2-2 | Quorum Standby | streaming |  1 |         0 |
|     3 | c-worker3-1 | c-worker3-1 | Leader         | running   |  1 |           |
|     3 | c-worker3-2 | c-worker3-2 | Quorum Standby | streaming |  1 |         0 |
+-------+-------------+-------------+----------------+-----------+----+-----------+
```



## Описание установки "ArenadataDB"

## Сравнительное тестирование 

## Выводы и планы на будущее
